
# Neural Network Model To Create a Binary Classification

Alphabet Soup, a fictatious venture capital firm requires a model that could predict whether a startup perspective borrower would become successful if funded by Alphabet Soup. The dataset consists of a csv file containing data of 34000 organizations that received funding from Alphhabet Soup over the years. The file contains various types of information about the organizations, including whether ultimately they become successful. 

After preprocessing of data, in this jupyter notebook, a Neural Network binary classifier model is built to make predictions about whether an applicant would become successful. Deep neural Network is built with TensorFlow.keras.Sequential model. Features of data to be used in the model are normalized through "standardscaler" and splitted into train_test proportion of 80-20.

Different techniques and several adjustments are applied to optimize the model and improve its accuracy. However, none of the techniques have resulted in better accuracy which probably demonstartes that more training data is required to improve the performance of model.

A highlight of various modifications in hyperparameters of the model are as below.

* The orignal model was started with 2 layers and number of neurons for hidden layers were equal to mean of input and output layer(1).
* The second model was altered with 3 hidden layers. This model chosen to increase the depth rather than making it more expansive. Deeper models are assumed to be robust and efficient (2).This model resulted in  slight better results than the original model.
* In third attempt, activation function is changed from 'relu' to 'LeakyRelU". With 3 hidden layers, number of Neurons were increased by 10% at every leayer. Additionally since batch sizes influences the model performance, its training time and generlization, therefore a batchsize of 5 was also added. This model's overall accuracy was slighly lower than the first two models but it had the better accuracy and loss score for testing data. 
* In fourth attempt, activation function was changed back to  'relu'. With same hyper parameters as they were in Model3,  to improve neural network's generalization ability l1 and l2 regulizors were adopted. Additionaly, a validation dataset was introduced to tells us how well the model is learning and adapting, before it's finally put to the test. Based upon the size of our data, the input for validation set was very low which could make the validation testing less significant to represent model's performance. 
* Further increase in number of epoches, number of batches did not perform well for these models.



|Accuracy Report                                              | Loss Performance                      |
| -----------------------------------                         | ----------------------------------- |
| ![image_1](accuracy_chart.png)                              | ![image_2](loss_chart.png) |



### Summary:

* Model 1 (Original):
   * Model Accuracy:    .7280
   * Model Loss    :    .5609

* Model 2:
   * Model Accuracy:    .7264
   * Model Loss    :    .5604

* Model 3:
   * Model Accuracy:    .7264
   * Model Loss    :    .5566
   
 * Model 4:
   * Model Accuracy:    .7287
   * Model Loss    :    .5541
   

### Conclusion:

Although all of the above models did not have much variance between training and testing data but Model 4 showed comparatively better predictions with accuracy of 72.87% and loss function is also lower than other models. This is not an optimal prediction since more than 1/3 of of the classifications generated by model are incorrect which could lead to selection of false applicants or deprive the company from true potential clients. Data augmentation or adding more data to train the model could probably improve the model accuracy, performance and predictions. 


References:
1. https://pub.towardsai.net/what-is-the-effect-of-batch-size-on-model-learning-196414284add
2. https://medium.com/@jacklindsai/why-is-deep-learning-deep-d4305e596b77


