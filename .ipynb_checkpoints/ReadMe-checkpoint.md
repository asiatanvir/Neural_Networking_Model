
# Neural Network Model To Create a Binary Classification

Alphabet Soup, a fictatious venture capital firm requires a model that could predict whether a startup perspective borrower would become successful if funded by Alphabet Soup. The dataset consists of a csv file containing data of 34000 organizations that received funding from Alphhabet Soup over the years. The file contains various types of information about the organizations, including whether ultimately they become successful. 

After preprocessing of data, in this jupyter notebook, a Neural Network binary classifier model is built to make predictions about whether an applicant would become successful. Deep neural Network is built with TensorFlow.keras.Sequential model. Features of data to be used in the model are normalized through "standardscaler" and splitted into train_test proportion of 80-20.

Different techniques and several adjustments are applied to optimize the model and improve its accuracy. However, none of the techniques have resulted in better accuracy which probably demonstartes that more training data is required to improve the performance of model.

A highlight of various modifications in hyperparameters of the model are as below.

* The orignal model was started with 2 layers and number of neurons for hidden layers were equal to mean of input and output layer(1). Produced results were not satisfactory.
* The second model was altered with 3 hidden layers. This model chosen to increase the depth rather making it more expansive. Deeper models are assumed to be robust and efficient (2).This model resulted in a slight improvement.
* In third attempt, activation function is changed from 'relu' to 'LeakyRelU".  Additionally since batch sizes influences the model performance, its training time and generlization, therefore a batchsize of 5 was also added. This model's overall accuracy was slighly lower than the first two models but it had the better accuracy and loss score for testing data.
* Further increasing the number of epoches, number of batches, number of neurorns resulted in decreasing the accuracy and loss performance.
* In fourth attempt, activation function is changed back to  'relu'. With same layers, neurons and batch size (as they were in third attempt), to improve neural network's generalization ability l1 and l2 regulizors were adopted. Additionaly, a validation dataset was introduced to tells us how well the model is learning and adapting, before it's finally put to the test.


|Accuracy Report                                              | Loss Performance                      |
| -----------------------------------                         | ----------------------------------- |
| ![image_1](accuracy_chart.png)                              | ![image_2](loss_chart.png) |



### Summary:

* Original model evaluation:
   * Model Accuracy:    .7264
   * Model Loss    :    .5581

* Model1:
   * Model Accuracy:    .7276
   * Model Loss    :    .5583 

* Model2:
   * Model Accuracy:    .7308
   * Model Loss    :    .5561
   
 * Model3:
   * Model Accuracy:    .7308
   * Model_Val_Accuracy:
   * Model Loss    :    .5561
   * Model_Val_loss : 



### Conclusion:

The third model generated comparatively better prediction accuracy of 73%. This is not an optimal prediction since more than 1/3 of of the classifications generated by model are incorrect which could lead to selection of false applicants or deprive the company from true potential clients. Data augmentation or adding more data to train the model could probably improve the model accuracy, performance and predictions. 


References:
1. https://pub.towardsai.net/what-is-the-effect-of-batch-size-on-model-learning-196414284add
2. https://medium.com/@jacklindsai/why-is-deep-learning-deep-d4305e596b77


